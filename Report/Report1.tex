\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{graphicx}
\graphicspath{ {Images/} }
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{array}
\usepackage{dcolumn} % automatically loads the 'array' package
\geometry{
	 a4paper,
	 total={210mm,297mm},
	 left=10mm,
	 right=10mm,
	 top=20mm,
	 bottom=15mm
 }

%For code pasting
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=1mm,
  belowskip=1mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcounter{debug}
\setcounter{debug}{1}

\titleformat{\chapter}[display]{\normalfont\bfseries}{}{0pt}{\Huge}
%Space before and after \chapter {}{before}{after}
\titlespacing*{\chapter}{0pt}{-50pt}{10pt}

\begin{document}

\begin{titlepage}
   \begin{center}
       %\vspace*{1cm}
       \LARGE
       \textbf{POLITECNICO DI TORINO}
       \vspace{1cm}
       
       \includegraphics[scale=0.2]{logoPolito-1.png}
       \vspace{0.8cm}
       
        \textbf{Computer aided simulations and performance evaluation}
       \vspace{1.5cm}
       
       Academic year 2020/21
       \vfill
       \begin{flushright}
       			\large
            VASSALLO Maurizio, s276961
       \end{flushright}
   \end{center}
\end{titlepage}

\tableofcontents

% ### REPORT 1 ###
\ifnum\value{debug}=1 {
    
\chapter{Queue Systems}
	These experiments are run to understand the behaviour of a queue system with a given number of servers (one or more), a capacity waiting line (finite or infinite) a FIFO (First In First Out) serving discipline and inter-arrivals time that follow a Poisson process.
	
	 \section{A single-server queuing system}
	 			\subsection{Exponential Distributed Service Time}
	 						\includegraphics[scale=0.6]{Lab5/QueueSystemRuns5.png} \\
								It is possible to see that the results obtained with simulation are close to the theoretical formula. 
			The theoretical value is calculated as:
			\begin{equation}
				theorical\_queue\_time = \frac{1}{ \mu - \lambda} 
			\end{equation}
			\begin{center}
				Where $\lambda=\frac{1}{ARRIVAL}$ and $\mu=\frac{1}{SERVICE}$
			\end{center}
			
	 			\subsection{??? Distributed Service Time}
	 			\includegraphics[scale=0.6]{Lab5/QueueSystemExponentialDistributionFiniteQueueSingleServer4Runs.png} \\
	 			\begin{equation}
						general\_theorical\_queue\_time = ro \times ( 1 + ro \times \frac{1+C_s^2}{2\times(1-ro)} )
				\end{equation}
				\begin{center}
						$ro=\frac{\lambda}{\mu}=LOAD$, $C_s^2$ is the coefficient of variation and depends on the service time distribution. \\
						This is the general formula indeed can be applied also to a service time with Exponential distribution ($C_s^2=1$); in the case of ??? distribution the $C_s^2=0$
				\end{center}
} \fi



% ### REPORT 2 ###
\ifnum\value{debug}=2 {
    
\chapter{Bins \& Balls}
	 
	\section{Introduction}
	 
	This experiment involves a number \emph{N} of bins and balls: for each ball we choose random bins, one or more than one, depending on the dropping policy, and we put the ball in one of those bins. The goal is to evaluate the maximum occupancy of the bins and compare the results with the theoretical ones. There are 2 dropping policy:
	\begin{itemize}
		\item Random Dropping: for each ball we choose a random bin and we put the ball in it; 
		\item Random Load Balancing: for each ball we choose \emph{d} random bins and we fill the one with a lower occupancy. In this simulation the values of \emph{d} used are 2 and 4.
	\end{itemize}

	\section{Tasks}
	 
		\subsection{Define all the input parameters of the simulator}
			The input parameters of the simulation are:
			\begin{itemize}
				\item Number N of bins and balls;
				\item The seed value used to initialize a pseudorandom number generator;
				\item The confidence level used to calculate the confidence interval;
				\item The number of runs: the number of times that we run our simulation. This is done in order to have more accurate results.
			\end{itemize}
			 
	\subsection{Define all the output metrics of the simulator}
			The output metrics of the simulation are:
			\begin{itemize}
				\item Number N of bins and balls;
				\item The theoretical lower bound value;
				\item The theoretical upper bound value;
				\item The lower confidence interval value;
				\item The average max occupancy value;
				\item The upper confidence interval value;
				\item The relative error.
			\end{itemize}
			All these value are stored in a file and each field is tab separated.
	
	\subsection{Define the main data structures for the simulator}
			The data structure used is an array where to store the bin occupancy at position \emph{i}. This data structure allows to have a constant access time ($\mathcal{O}(1)$) and also a worst-case constant access time ($\mathcal{O}(1)$).
			
	\subsection{Describe the architecture of the simulator in terms of source files and output files}
			The whole simulation runs inside a script where the simulator runs multiple times for multiple values of bins and balls. \\
			The output is a .dat file that contains the output metrics, therefore this file will contain a number of lines equal to the number of different values of bins and balls used. This .dat file is then elaborated by scripts in order to create some plots. There are 3 scripts for plotting:
			\begin{itemize}
				\item One plots the results of the simulation to have a comparison with the theoretical occupancy values;
				\item One plots the different performances of the dropping policy;
				\item One plots the relative errors for different values of the number of runs.
			\end{itemize}
			
			\subsection{For the random dropping policy, show the coherence of the simulation results with the theoretical bounds, for N in [100; 10\textsuperscript{6}]. Show the effect of varying the number of runs for each experiment on the relative error}
			
			\includegraphics[scale=0.6]{Lab1/RandomDroppingPolicyRuns3.png} \\
			It is possible to see that 3 runs are not enough since the confidence interval is not inside the theoretical values.
			The lower bound is calculated using the formula:
			
			\begin{equation} \label{eq:1}
				 \text{max\_occupacy = }\frac{\log n}{\log \log n} 
			\end{equation}
			\begin{center}
					where \emph{n} is the number of bins and balls.
			\end{center}
			While the upper bound is just 3 times this formula.
			
			\includegraphics[scale=0.6]{Lab1/RandomDroppingPolicyRuns5.png} \\
			It is possible to see that 5 runs are enough since the confidence interval is inside the theoretical values.
			
			\vspace{20px}
			
			\includegraphics[scale=0.6]{Lab1/RandomLoadBalancingd2Runs3.png} \\
			Even with 3 runs the Load Balancing with \emph{d=2} is near to the theoretical value.
			\begin{equation} \label{eq:2}
				 \text{max\_occupacy = }\frac{\log \log n}{\log d}
			\end{equation}
			\begin{center}
					where \emph{n} is the number of bins and balls and \emph{d} the number of random bins chosen.
			\end{center}
			It is possible to see that this formula is much smaller than formula \ref{eq:1}, so we expect a lower occupacy value.
			
			\vspace{20px}
			
			\includegraphics[scale=0.6]{Lab1/RandomLoadBalancingd4Runs3.png} \\
			Even with 3 runs the Load Balancing with \emph{d=4} is quite near to the theoretical value. 
			It is possible to see that the average occupacy is lower with respect to the Load Balancing with \emph{d=2} (The 2 plots have the same y-axis scale).
			
			\includegraphics[scale=0.6]{Lab1/ComparisonamongPolicies,Runs=5.png} \\
			With this graph it is possible to see the differences between the policies and in particular: the Balacing policy works better than the Random one and that with an increasing number of bins selected the maximum occupancy decreases. \\ There must be a trade-off between the number of bins selected and the script execution time because with large values of \emph{d} the occupancy decreases but the execution time increases: in the extreme case with \emph{d=\#bins} we would put the ball in the least occupied bins (equal to execute \emph{np.argmin(bins)}) and have an average maximum occupancy of 1 (one ball in each bin)  but that would require some time, especially for large number of bins.
			
			\includegraphics[scale=0.6]{Lab1/RelativeErrorsRandomDroppingPolicy.png} \\
			It is possible to see that the relative errors decrease with increasing the number of runs. 
			\begin{equation}\label{eq:3}
				 \text{rel\_error = }2\frac{\Delta}{x}
			\end{equation}
			\begin{center}
					where $\Delta$ is the \emph{CI} length.
			\end{center}
			Similar results are obtained with the Loading Balancing policy; graphs are omitted.
					
			
\chapter{Birthday Paradox}
	 
	 \section{Introduction}
	 
	This experiment involves a number \emph{m} people: for each person a random number, the birthday in the case of Birthday Paradox, is chosen among \emph{n} possible values. The goal is to evaluate:
	\begin{itemize}
		\item The probability of conflict. A conflict is experienced two people have the same random number (same birthday);
		\item The minimum number of people required for a conflict.
	\end{itemize}

	\section{Tasks}
	 
		\subsection{Define all the input parameters of the simulator}
			The input parameters of the simulation are:
			\begin{itemize}
				\item Number of possible ``days'': in this simulation, this value can be: [365, $10^5$, $10^6$];
				\item The seed value used to initialize a pseudorandom number generator;
				\item The confidence level used to calculate the confidence interval;
				\item The number of runs: the number of times that we run our simulation. This is done in order to have more accurate results;
				\item A flag depending if we want to calculate the conflict probability or the minimum number of people needed to experience a conflict.
			\end{itemize}
			 
	\subsection{Define all the output metrics of the simulator}
			The output metrics of the simulation are:
			\begin{enumerate}
					\item Conflict Probability:
					\begin{itemize}
							\item Number N of persons;
							\item The lower confidence interval value;
							\item The average max occupancy value;
							\item The upper confidence interval value;
							\item The relative error.
					\end{itemize}
					\item Minimum number of people:
					\begin{itemize}
							\item The lower confidence interval value;
							\item The average max occupancy value;
							\item The upper confidence interval value;
							\item The relative error;
							\item The theoretical value.
					\end{itemize}
			\end{enumerate}
			All these value are stored in a file and each field is tab separated.
	
	\subsection{Define the main data structures for the simulator}
				The data structure used is a binary array to store whether the element (day in the case of Birthday Paradox) at position \emph{i} is occupied or not (0 means free, 1 means occupied: in the case of Birthday Paradox, in that day there is already, at least, one person who was born in that day). \\ This data structure allows to have a constant access time ($\mathcal{O}(1)$) and also a worst-case constant access time ($\mathcal{O}(1)$). \\
				For the calculation of the probability there is a counter that keeps how many conflicts happened. This is used to calculate the probability as $prob(conf)=\frac{\#conflicts}{\#people}$ at each run. \\
				%For the minimum number of people there is a an array storing the minimum number of people for a conflict at each run.
			
	\subsection{Describe the architecture of the simulator in terms of source files and output files}
			The whole simulation runs inside a script where the simulator runs multiple times for multiple values of people or just one in the case of finding the minimum number of people for a conflict. \\
			At each iteration the output is a .dat file that contains the output metrics, therefore this file will contain a number of lines equal to the number of different values of people or just one line in the case of minimum case. This .dat file is then elaborated by a script to create the plots.
		
		\subsection{Show the graph comparing the conflict probability with respect to the theoretical value,
for each value of m.}
				\includegraphics[scale=0.6]{Lab2/ProbabilityConflictDays365andRuns200.png} \\
				\includegraphics[scale=0.6]{Lab2/ProbabilityConflictDays100000andRuns250.png} \\
				The results obtained with 200 runs were not satisfying so the number of runs is increased to 250. \\
				\includegraphics[scale=0.6]{Lab2/ProbabilityConflictDays1000000andRuns250.png} \\
				Even with 180 runs the simulation is not very close to the theoretical result. \\
				\includegraphics[scale=0.6]{Lab2/ProbabilityConflictDays1000000andRuns600.png} \\
				Better results can be obtained increasing a lot the number of runs but such a huge number of runs requires more time to be executed.
				
		\subsection{Is the theoretical formula for the conflict probability accurate?}
				Yes, how it can be seen from the previous graphs, the formula is accurate but for large number of days it is less accurate and it requires an higher number of runs.
				
		\subsection{Is it possible to estimate a-priori the range of m such that the probability in the graph covers all the range in [0; 1]?}
				Yes, it is possible using the following formula:
				\[
						num\_elements \: = \: \sqrt{2 \times number\_days \times \log{ \left( \frac{1}{1-p} \right) } }
				\]
				\begin{center}
							where \emph{p} is the wanted probability, $ p \in [0,1)$.
				\end{center}
		With this formula it is possible to find all values of \emph{m} given a value of \emph{p}, the only value non possible to find is \emph{p=1} since this is not allowed in the formula (division by 0) but we have $100\%$ probability of conflict if $number\_days+1$ elements (people) are chosen.
		
		\subsection{Fill the following table and comment about the accuracy of the theoretical formula.}
				\begin{table}[h]
							\centering
							%|l|l|l|l|l|l|
							\begin{tabular}{|c|c|c|c|c|c|c|}
							\hline
							\multicolumn{1}{|c|}{\textbf{n}} & 
							\multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}\# runs\end{tabular}}} & 
							\multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Average number of people\\ needed for a conflict\end{tabular}}} & 
							\multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}95\% confidence\\ intervals\end{tabular}}} & 
							\multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Theoretical\\ value\end{tabular}}} & 
							\multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}MAE\end{tabular}}}\\ \hline
							%							days				runs								simul												CI						theor										MAE
							\textbf{365}            & 200 		& 22.64									& 1.63      & 23.94									& 1.30										 					\\ \hline
							\textbf{100000}       &	250			& 401.72       & 27.13    & 396.33      	& 5.39								\\ \hline
							\textbf{1000000}     &	250			& 1154.26     & 79.76				&	1253.31      & 99.05							\\ \hline
							\textbf{1000000}     &	400			& 1253.31     & 63.19			&	1253.31      & 53.76							\\ \hline
							\end{tabular}
				\end{table}
			The number of runs are chosen such that the relative errors (\ref{eq:3}) were under $0.15$ but in case of $10^6$ days the relative error is a bit smaller than the others ($0.11$ vs $0.14$) since with $250$ runs the MAE value was greater than the CI value.

} \fi
			
			
			
% ### REPORT 3 ###
\ifnum\value{debug}=3 {

\chapter{Laboratory \#3}
	 \subsubsection{Introduction}
				This experiment involves the usage of the English dictionary to check the performance of different data structures when dealing with the membership problem, so answer the question: `'is an element already present?''. 
	 
	 \section{Fingerprinting}	 
	 
			 	\subsection{Let w be the number of words available in the file. What is w?}
								The English word dictionary contains $370103$ words.
								
			 	\subsection{By simulation, compute the minimum value of bits $b^{exp}$ such that no collisions are experienced when storing the whole list of English words in a fingerprint set}
						The minimum number of bits necessary to store all the words in a data structure without conflicts is: $b^{exp} = 39$ bits.
						
				\subsection{Define all the input parameters of the simulator}
					The input parameters of the simulation are:
					\begin{itemize}
						\item The file containing the English words;
						%\item An upper bound for the number of bits. This will be used in a Binary Search algorithm to find the minimum value of bits $b^{exp}$ such that no collisions are experienced.
					\end{itemize}
					 
				\subsection{Define all the output metrics of the simulator}
					The output metrics of the simulation are:
					\begin{enumerate}
							\item The value of $b^{exp}$;
							\item The theoretical value of b: $b^{teo}$;
							\item The storage  memories required for the data structures and the theoretical storage memories;
							\item The probability of false positive using a  $b^{exp}$ fingerprint set.
					\end{enumerate}
			
			\subsection{Define the main data structures for the simulator}
						There are 2 main data structures used: 
							\begin{itemize}
								\item A python set where to store all the English words;
								\item A python set where to store the fingerprint of each English word.
								\end{itemize}
						After storing all the words, to find the minimum value of bits $b^{exp}$ such that no collisions are experienced, for each one of the words, a fingerprint is calculated:
						\begin{itemize}
								\item[] Each word is encode using the UTF-8 character encoding, then the MD5 hash is calculated. This MD5 hash returns a 128 bits value but since it is required to have fingerprint length is $b^{exp}$ bits, the MD5 hash value is converted to integer and only the last  $b^{exp}$ bits are taken; this is done with the module operation: \[ fingerprint\_value = word\_hash\_int \: \% \: 2^{b^{exp}} \]
						\end{itemize}
						This operation is repeated for each word and candidate $b^{teo}$ value until the minimum value possible is found. In particular this search is performed using a Binary Search algorithm; to write is a bit more complex than other algorithms but it is faster: for example is faster than an algorithm that checks all numbers between 1 and infinite and stops as soon it find a value of $b^{exp}$ such that any collision is found.
					
			\subsection{Compute analytically the number of bit $b^{teo}$ necessary to get a probability $0.5$ of fingerprint collision when building the dictionary with a fingerprint set}
					It is possible to analytically compute the minimum number of bits given a specified probability\emph{p} using the following formula:
					\[
							b^{teo} \: = \: \log_2{ \left( - \frac{num\_words^2}{2\times \ln\left({1-p}\right)} \right)}
					\]
					
			\subsection{How $b^{exp}$ and $b^{teo}$ are related?}
					The theoretical value and the simulated value are really close: 
					\[
							b^{exp} \: = \: 39; \; 	b^{teo} \: = \: 37 \: \text{(the decimal value is 36.52), ratio = 39/37 = 1.07}
					\]
			
			\subsection{Evaluate the theoretical minimum amount of memory necessary to store the whole dictionary in a $b^{exp}$-fingerprint set and in a word set}
			The theoretical memory necessary, in MB, to store all the words using the two structures, fingerprint table and python list, would be:
			\[ theoretical\_size\_fp\_table \: = \: num\_words \times \frac{b^{exp}}{8} \times \frac{1}{1024^2}\: = \: 1.72 \text{MB}\] 
			\[ theoretical\_size\_python\_set \: = \: num\_words \times 4.7 \times \frac{1}{1024^2} \: = \: 1.66 \text{MB}\] 
			where $4.7$ is the average length words of the English dictionary (1 character = 8 bits) (\href{http://norvig.com/mayzner.html}{reference}).
						
			\subsection{Measure the actual amount of memory necessary to store the whole dictionary in a $b^{exp}$ fingerprint set and in a word set in your python implementation}
			To calculate the memory used by an object in python it is possible to use the \emph{asizeof(obj)} from the \emph{pympler} library. This returns the memory in bytes of a given object.
			\begin{center}
					Memory required to store the fingerprint table: 16.47 MB, for the set of words: 21.46 MB
			\end{center}
			
			\subsection{Assume a spell checker application in which a given word is compared with the words in the dictionary. Compute the probability of false positive for the $b^{exp}$-fingerprint set}
			There are different ways to calculate the probability of false positive, simulation is one of this. But in this case is not needed since it is possible to calculate the probability of false positive analytically, using the following formula:
			\begin{center}
					prob(false\_pos) = number\_words / $2^{b^{exp}}$
			\end{center}
			
			\subsection{To summarize all the previous results.}

				\begin{table}[h!]
				\renewcommand{\arraystretch}{1.35}
				\centering
						\begin{tabular}{|l|c|c|c|c|}
						\hline
						\multicolumn{1}{|c|}{\textbf{Storage}} &
						  \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Bits per \\ fingerprint\end{tabular}}} &
						  \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Prob. \\ false positive\end{tabular}}} &
						  \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Min theoretical \\ memory (MB)\end{tabular}}} &
						  \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Actual memory\\ (MB)\end{tabular}}} \\ \hline
						\textbf{Word set} &
						  N.A. &
						  $ 1.591 \times 10^{-7}$ &
						  1.66 &
						  21.46 \\ \hline
						\textbf{Fingerprint set} &
						  39 &
						  0* &
						  1.72 &
						  16.47 \\ \hline
						\end{tabular}
				\end{table}
				It is possible to see that  the memory required for the python set is much more the memory required to store the fingerprint table, this difference is more evident if the elements to be stored are different in sizes, for example: to store audio files, the memory required for the python set would be very high instead the one for the fingerprint table would not change (as long as we keep the same value of number of bits). \\
				The probabilities of false positive is calculated using $ 5 \times 10^{6}$ number of testing words (words not in the English dictionary) but the probability of false positive for the fingerprint table is still 0. \\
				* This is because it is a rare event and would require even an higher number of test words but this would require more time and power so in this case it is better to calculate analytically with the following formula:
				\begin{center}
					prob(false\_pos) = number\_words / $2^{39} =  6.732 \times 10^{-7}$
			\end{center}
				
			\newpage
					
			
			\section{Bit String Array}	 
	 
				\subsection{Define all the input parameters of the simulator}
					The input parameters of the simulation are:
					\begin{itemize}
						\item The file containing the English words;
						\item A list with the possible values of the number of bits: [19, 20, 21, 22, 23, 24];
						\item The number of words are used to check the probability of false positive;
						\item A flag to indicate that we want to use Bit String Array, Bloom Filter or Counting Bloom Filter. (0:Bit String Array);
						\item The seed value used to initialize a pseudorandom number generator;
						\item The confidence level used to calculate the confidence interval;
						\item The number of runs: the number of times that we run our simulation. This is done in order to have more accurate.
					\end{itemize}
					 
				\subsection{Define all the output metrics of the simulator}
				The output metrics of the simulation are:
				\begin{itemize}
					\item Number of bits;
					\item The lower confidence interval value;
					\item The average probability of false positive;
					\item The upper confidence interval value;
					\item The relative error;
					\item The theoretical probability of false positive;
					\item The memory occupancy of the bit string array;
				\end{itemize}
				All these value are stored in a file and each field is tab separated.
			
			\subsection{Define the main data structures for the simulator}
						There are 3 main data structures used: 
							\begin{itemize}
								\item A python list to store all the English words;
								\item A bit string array,  from the \emph{bitarray} library, of length $2^{\#bits}$;
								\item A numpy array, of length \#runs, where to store the probability of false positive at each run.
							\end{itemize}
					The bit string array allows to determine if an element is already stored or not with a constant access time of $\mathcal{O}(1)$, since it share the same property of a simple array.
					
			\subsection{Show in a graph the probability of false positive in function of the number of bits per fingerprint.}
			\includegraphics[scale=0.6]{Lab3/BitStringArrayProbabilityFalsePositiveRuns5.png} \\
			The simulation is run with using 20000 `fake' words and even with such a lower number the simulated results are quite accurate. \\
			The theoretical value is calculated as:
				%prob(arr[H(w)]=1|H(w) \not\in arr) \: = \: \frac{\#1s}{total\_length} 
			\begin{equation}\label{eq:1}
						prob(false\_pos) \: = \: \frac{\#1s}{total\_length}
			\end{equation}
			%Where arr is the bit string array, w is the word not in the english vocabulary, $H(\cdot)$ is the hash function and $tota\_length$ is the total size of the bit string array.
			Where $\#1$ is the number of 1s in the bit string array and $total\_length$ is the total size of the bit string array.
			
			%\subsection{Measure the actual memory occupancy in function of the number of bits per fingerprint.}
			%See next table..
			
			%\newpage 
			
			\subsection{To summarize all the previous results, fill the following table and compare these results with the ones obtained in the table of Sec. 2.1.1}
			\renewcommand{\arraystretch}{1.25}
			\begin{table}[h!]
			\centering
					\begin{tabular}{|c|c|c|c|c|}
					\hline
					\textbf{Storage} &
					  \textbf{\begin{tabular}[c]{@{}c@{}}Bits per \\ fingerprint\end{tabular}} &
					  \textbf{\begin{tabular}[c]{@{}c@{}}Prob. \\ false positive\end{tabular}} &
					  \textbf{\begin{tabular}[c]{@{}c@{}}Min theoretical \\ memory (MB)\end{tabular}} &
					  \textbf{\begin{tabular}[c]{@{}c@{}}Actual\\ memory (MB)\end{tabular}} \\ \hline
						\multicolumn{1}{|l|}{\textbf{Word set}}                                              & N.A.        & 0           & 1.66 & 15.28  \\ \hline
						\multicolumn{1}{|l|}{\textbf{Fingerprint set}}                                       & 36          & $1.077 \times 10^{-5}$ & 1.59 & 16.02  \\ \hline
						\multicolumn{5}{|l|}{}                                                                                                           \\ \hline
						\multirow{6}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Bit String\\ Array\end{tabular}}} 
																																																											& 19 & 0.506        & 0.044/0.066 & 0.062   \\ \cline{2-5} 
						                                                                                     & 20 & 0.297        & 0.044/0.132 & 0.125   \\ \cline{2-5} 
						                                                                                     & 21 & 0.162        & 0.265 & 0.25  \\ \cline{2-5} 
						                                                                                     & 22 & 0.084        & 0.531 & 0.5  \\ \cline{2-5} 
						                                                                                     & 23 & 0.043        & 1.062 & 1.0  \\ \cline{2-5} 
						                                                                                     & 24 & 0.022        & 2.125 & 2.0 \\ \hline
				\end{tabular}
		\end{table}
		The theoretical probability is calculated as the memory required to store the number of words using a 1 bit (possible values: True, False) so it does not depend on the number of bit used. Formula:
		\[ theoretical\_memory \: = \: num\_words \times \frac{1}{8} \times \frac{1}{1024^2} \: = \: 0.044 \text{MB}\]
		
		
		\newpage
		
					\section{Bloom Filters}	 
	 				The Bloom Filter is just an extension of the Bit String Array so they share lot of code. 
				\subsection{Define all the input parameters of the simulator}
					The input parameters of the simulation are:
					\begin{itemize}
						\item The same as the bit string array;
						\item A flag to indicate that we want to use Bit String Array, Bloom Filter or Counting Bloom Filter: (1:Bloom Filter);
					\end{itemize}
					 
				\subsection{Define all the output metrics of the simulator}
				The output metrics of the simulation are:
				\begin{itemize}
					\item The same as the bit string array;
					\item The number of hashes used.
				\end{itemize}
			For Bloom Filters, the time needed to determine if an element is already stored or not is constant and it has a worst case access time of $\mathcal{O}(k)$, where k is the number of hash functions used.
			
			\subsection{Define the main data structures for the simulator}
						The same as the Bit String Array.
					
			\subsection{Compare in a graph the measured probability of false positive and the theoretical one}
			\includegraphics[scale=0.6]{Lab3/BloomFilterProbabilityFalsePositiveRuns5.png} \\
			The simulation is run with using 10000 `fake' words and even with such a lower number the simulated results are quite accurate. \\
			The theoretical value is calculated the a similar formula as the bit string array (\ref{eq:1}):
			\begin{equation} \label{eq:2}
				prob(false\_pos) \: = \: \left( \frac{\#1s}{total\_length} \right)^{k_{opt}}
			\end{equation}
			Where \emph{k} is the number of different values of hashes and is calculated as: 
			
			\begin{equation} \label{eq:3}
					k_{opt} = \frac {2^{\#bits}}{number\_words} \times \ln(2)
			\end{equation}
			Given this value, one should check both upper and lower integer( ceil() and floor() ) and pick the best one but usually truncate to the near integer works fine.
			
			%\subsection{Measure the actual memory occupancy in function of the number of bits per fingerprint.}
			%See next table..
			
			\newpage 
			
			\subsection{To summarize all the previous results, fill the following table and compare these results with the ones obtained in the table of Sec. 2.1.1 and Sec. 2.2.1.}
%			\begin{table}[h!]
%				\resizebox{\textwidth}{!}{%
%					\begin{tabular}{|c|c|c|c|c|}
%					\hline
%					\textbf{Storage} &
%					  \textbf{\begin{tabular}[c]{@{}c@{}}Bits per \\ fingerprint\end{tabular}} &
%					  \textbf{\begin{tabular}[c]{@{}c@{}}Prob. \\ false positive\end{tabular}} &
%					  \textbf{\begin{tabular}[c]{@{}c@{}}Min theoretical \\ memory (MB)\end{tabular}} &
%					  \textbf{\begin{tabular}[c]{@{}c@{}}Actual\\ memory (MB)\end{tabular}} \\ \hline
%						\multicolumn{1}{|l|}{\textbf{Word set}}                                              & N.A.        & 0           & 1.66 & 15.28  \\ \hline
%						\multicolumn{1}{|l|}{\textbf{Fingerprint set}}                                       & 36          & $1.077 \times 10^{-5}$ & 1.59 & 16.02  \\ \hline
%						\multicolumn{5}{|l|}{}                                                                                                           \\ \hline
%						\multirow{6}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Bit String\\ Array\end{tabular}}} 
%																																																											& 19 & 0.49        & 0.83 & 0.50   \\ \cline{2-5} 
%						                                                                                     & 20 & 0.27        & 0.88 & 1.00   \\ \cline{2-5} 
%						                                                                                     & 21 & 0.14        & 0.92 & 2.00  \\ \cline{2-5} 
%						                                                                                     & 22 & 0.07        & 0.97 & 4.00  \\ \cline{2-5} 
%						                                                                                     & 23 & 0.03        & 1.01 & 8.00  \\ \cline{2-5} 
%						                                                                                     & 24 & 0.01        & 1.05 & 16.00 \\ \hline
%				\end{tabular}
%				}
%		\end{table}
		
		%\vspace{-0.7cm}
		
		\renewcommand{\arraystretch}{1.25}
		\begin{table}[h!]
		\centering
				\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				\textbf{Storage} &
				  \textbf{\begin{tabular}[c]{@{}c@{}}Bits per \\ fingerprint\end{tabular}} &
				  \textbf{\begin{tabular}[c]{@{}c@{}}Optimal number \\ hash functions\end{tabular}} &
				  \textbf{\begin{tabular}[c]{@{}c@{}}Prob. \\ false positive\end{tabular}} &
				  \textbf{\begin{tabular}[c]{@{}c@{}}Min theoretical \\ memory (MB)\end{tabular}} &
				  \textbf{\begin{tabular}[c]{@{}c@{}}Actual\\ memory (MB)\end{tabular}} \\ \hline
				\multirow{6}{*}{\textbf{Bloom Filter}}
																												& 19 & 1  & 0.49  & 0.83 & 0.50/8   \\ \cline{2-6} 
				                                       & 20 & 2  & 0.27  & 0.88 & 1 .00  \\ \cline{2-6} 
				                                       & 21 & 4  & 0.05  & 0.93 & 2.00  \\ \cline{2-6} 
				                                       & 22 & 6  & 0.001 & 0.97 & 4.00  \\ \cline{2-6} 
				                                       & 23 & 16 & 0     & 1.01 & 8.00  \\ \cline{2-6} 
				                                       & 24 & 31 & 0     & 1.06 & 16.00 \\ \hline
				\end{tabular}
				
				\end{table}
				
				\subsection{Given 1 MB of available memory for the storage, what are the performance of the spell checker application using the word set, the fingerprint set, the bit string array and the bloom filter?}
				Theoretically, given 1 MB of memory the performance using the different data structures are:
				\begin{itemize}
					\item Word set: the storage is not possible since we may only store, $\frac{1MB}{1.66MB}=0.047$, 4.7\% of the total words;
					\item[] For the other structures we could only use a number of bits equal to $\frac{1024^2*8}{\#words}=22.66$ bits, so approximating this value, only 22 bits given 1MB of space.
					\item Fingerprint set: the storage of the whole list of words is always possible but there is a change on the probability of false positive depending on the number of bits used. Using 22 bits the probability of false positive would be $eps=\frac{\#words}{2^{22}}=0.0882$; 
					\item Bit String Array: Using 22 bits the probability of false positive would be $eps=\frac{1}{22}=0.0454$;
					\item Bloom filter: Using 22 bits the probability of false positive would be $eps=\frac{1}{2^{\frac{22}{1.44}}}=2.51 \times 10^{-5}$. 
				\end{itemize}
				

			\renewcommand{\arraystretch}{1.35}
				\begin{table}[h!]
				\centering
					\resizebox{0.8\textwidth}{!}{%
						\begin{tabular}{|c|l|c|l|l|l|}
						\hline
						\textbf{\begin{tabular}[c]{@{}c@{}}Data\\ Structure\end{tabular}} &
						  \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Total Storage\\ Formula (bit)\end{tabular}}} &
						  \multirow{4}{*}{} &
						  \textbf{\begin{tabular}[c]{@{}c@{}}Max\\ Num\\ bits\end{tabular}} &
						  \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Formula solved \\ for Epsilon\end{tabular}}} &
						  \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Epsilon\\ Value\end{tabular}}} \\ \cline{1-2} \cline{4-6} 
						\textbf{Fingerprint Table} & \multicolumn{1}{c|}{$m\times \log_2{ \frac{m}{eps} }$}     &  & \multirow{3}{*}{22}  & \multicolumn{1}{c|}{$\frac{m}{2^{22}}$}      									& \multicolumn{1}{c|}{0.0882}      																	\\ [5px]  \cline{1-2} \cline{5-6} 
						\textbf{Bit String Array}  	 & \multicolumn{1}{c|}{$m \times \frac{1}{eps}$} 										   &  &                     							 & \multicolumn{1}{c|}{$\frac{1}{22}$}      														& \multicolumn{1}{c|}{0.0454}      																	\\ [5px] \cline{1-2} \cline{5-6} 
						\textbf{Bloom Filter}      	 & \multicolumn{1}{c|}{$m \times 1.44\log_2{\frac{1}{eps}}$} 		&  &                     							 & \multicolumn{1}{c|}{$ \frac{1}{2^{\frac{22}{1.44}}} $} 		& \multicolumn{1}{c|}{$2.51 \times 10^{-5}$} 						\\ [5px] \hline 
						\end{tabular}
						}
				\end{table}
				Where \emph{m} is the number of words to be stored.
%		\begin{table}[h!]
%				\centering
%					\resizebox{\textwidth}{!}{%
%				\begin{tabular}{|c|c|c|}
%				\hline
%				\textbf{\begin{tabular}[c]{@{}c@{}}Data\\ Struncture\end{tabular}} &
%				  \textbf{\begin{tabular}[c]{@{}c@{}}Total Storage\\ Formula (bit)\end{tabular}} &
%				  \textbf{\begin{tabular}[c]{@{}c@{}}Epsilon\\ for 1MB\end{tabular}} \\ \hline
%				\textbf{Fingerprint Table} & $m \times \log_2{ \frac{m}{eps} }$ & 0.0882 \\ \hline
%				\textbf{Bit String Array}  & $m \times \frac{1}{eps}$  											& 0.0454 \\ \hline
%				\textbf{Bloom Filter}      & $m \times 1.44\log_2{\frac{1}{eps}}$ & $2.51 \times 10^{-5}$ \\ \hline
%				\end{tabular}
%				}
%		\end{table}


				\subsection{(optional) For each value of the number of bits per fingerprint, show in a graph the probability of false positive in function of the number of hash functions, in the range 1 - 32. Is the optimal value of the number of hash function obtained by simulation coherent with the theory?}
				\includegraphics[scale=0.6]{Lab3/BloomFiltersProbabilityFalsePositive_0_6.png} \\
				It is possible to see that the theoretical
%				\begin{minipage}{0.4\textwidth}
%						\includegraphics[scale=0.45]{Lab3/BloomFiltersProbabilityFalsePositive_0_3.png}
%				\end{minipage}
%				\begin{minipage}{0.4\textwidth}
%						\includegraphics[scale=0.4]{Lab3/BloomFiltersProbabilityFalsePositive_3_6.png}
%				\end{minipage}
				
				\subsection{(optional) It can be shown that the number of distinct elements stored in a bloom filter
with n bits and k hash functions can be estimated in function of N1, the actual number of bits equal to 1 in the bloom filter. Show how accurate is the formula when inserting word-by-word the dictionary, in function of the inserted words.}
				\includegraphics[scale=0.6]{Lab3/ComparisonTheoryvsSimulationnumberdistinctWords_11.png} \\
				Given the theoretical formula: 
				\[ theo\_dist\_words = - \frac{n}{k} \ln \left( 1 - \frac{N_1}{n} \right) \]
				\begin{center}
				Where: n is the bloom filter storage length , k is the optimal number of hash functions given a number of bit  (\ref{eq:3}) and $N\_1$ is the actual number of 1s in the bloom filter. 
				\end{center}
				It is possible to see that the simulation is quite accurate wrt the theoretical formula. This is shown using the relative errors ($ rel\_err=\frac{|theo\_dist\_words-dist\_words|}{dist\_words} $) and these relative errors are low (the axis scale is quite low) so the theoretical formula is accurate.
				
} \fi

% ### REPORT 3 ###
\ifnum\value{debug}=3 {
} \fi

\end{document}